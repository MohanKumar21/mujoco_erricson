{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.ppo    import PPO\n",
    "from agents.agent            import Agent\n",
    "\n",
    "\n",
    "from discriminators.eairl    import EAIRL\n",
    "from utils.utils             import RunningMeanStd, Dict, make_transition\n",
    "\n",
    "from configparser            import ConfigParser\n",
    "from argparse                import ArgumentParser\n",
    "\n",
    "import os\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 500 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:39: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "<>:92: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "<>:39: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "<>:92: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "/var/folders/30/6_15k8xn4kn7vwrk6x8py9sc0000gq/T/ipykernel_3337/2565607335.py:39: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "  if encode_method is 'positions':\n",
      "/var/folders/30/6_15k8xn4kn7vwrk6x8py9sc0000gq/T/ipykernel_3337/2565607335.py:92: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "  if encode_method is 'positions':\n"
     ]
    }
   ],
   "source": [
    "action_dim = 1\n",
    "state_dim = env.observation_space.n\n",
    "print(action_dim, state_dim, action_dim)\n",
    "print()\n",
    "parser = ArgumentParser('parameters')\n",
    "# Taxi specific \n",
    "def decode_positions(states):\n",
    "    \"\"\"\n",
    "    Gets an state from env.render() (int) and returns\n",
    "    the taxi position (row, col), the passenger position\n",
    "    and the destination location\n",
    "\n",
    "    :param states: a list of states represented as integers [0-499]\n",
    "    :return: taxi_row, taxi_col, pass_code, dest_idx\n",
    "    \"\"\"\n",
    "    dest_loc = [state % 4 for state in states]\n",
    "    states = [state // 4 for state in states]\n",
    "    pass_code = [state % 5 for state in states]\n",
    "    states = [state // 5 for state in states]\n",
    "    taxi_col = [state % 5 for state in states]\n",
    "    states = [state // 5 for state in states]\n",
    "    taxi_row = states\n",
    "    return taxi_row, taxi_col, pass_code, dest_loc\n",
    "def encode_states(states, encode_method, state_dim):\n",
    "    \"\"\"\n",
    "        Gets a list of integers and returns their encoding\n",
    "        as 1 of 2 possible encoding methods:\n",
    "            - one-hot encoding (array)\n",
    "            - position encoding\n",
    "\n",
    "    :param states: list of integers in [0,num_states-1]\n",
    "    :param encode_method: one of 'one_hot', 'positions'\n",
    "    :param state_dim: dimension of state (used for 'one_hot' encoding)\n",
    "    :return: states_encoded: one hot encoding of states\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = len(states)\n",
    "\n",
    "    if encode_method is 'positions':\n",
    "        '''\n",
    "            position encoding encodes the important game positions as \n",
    "            a 19-dimensional vector:\n",
    "                - 5 dimensions are used for one-hot encoding of the taxi's row (0-4)\n",
    "                - 5 dimensions are used for one-hot encoding of the taxi's col (0-4)\n",
    "                - 5 dimensions are used for one-hot encoding of the passenger's position:\n",
    "                    0 is 'R', 1 is 'G', 2 is 'Y', 3 is 'B' and 4 is if the passenger in the taxi\n",
    "                - 4 dimensions are used for one-hot encoding of the destination location:\n",
    "                    0 is 'R', 1 is 'G', 2 is 'Y' and 3 is 'B'\n",
    "                we simply concatenate those vectors into a 19-dim. vector with 4 ones in it\n",
    "                corresponding to the positions encoding and the rest are zeros.      \n",
    "        '''\n",
    "\n",
    "        taxi_row, taxi_col, pass_code, dest_loc = decode_positions(states)\n",
    "\n",
    "        # one-hot encode taxi's row\n",
    "        taxi_row_onehot = np.zeros((batch_size, 5))\n",
    "        taxi_row_onehot[np.arange(batch_size), taxi_row] = 1\n",
    "        # one-hot encode taxi's col\n",
    "        taxi_col_onehot = np.zeros((batch_size, 5))\n",
    "        taxi_col_onehot[np.arange(batch_size), taxi_col] = 1\n",
    "        # one-hot encode row\n",
    "        pass_code_onehot = np.zeros((batch_size, 5))\n",
    "        pass_code_onehot[np.arange(batch_size), pass_code] = 1\n",
    "        # one-hot encode row\n",
    "        dest_loc_onehot = np.zeros((batch_size, 4))\n",
    "        dest_loc_onehot[np.arange(batch_size), dest_loc] = 1\n",
    "\n",
    "        states_encoded = np.concatenate([taxi_row_onehot, taxi_col_onehot,\n",
    "                                         pass_code_onehot, dest_loc_onehot], axis=1)\n",
    "\n",
    "    else:   # one-hot\n",
    "        states_encoded = np.zeros((batch_size, state_dim))\n",
    "        states_encoded[np.arange(batch_size), states] = 1\n",
    "\n",
    "    return states_encoded\n",
    "# Define all the model parameters\n",
    "hidden_units = 32               # num. units in hidden layer\n",
    "replay_buffer_size = 200000     # buffer size\n",
    "start_learning = 50000          # num. transitions before start learning\n",
    "target_update_freq = 10000      # num. transitions between Q_target network updates\n",
    "eps = 0.1                       # final epsilon for epsilon-greedy action selection\n",
    "schedule_timesteps = 350000     # num. transitions for epsilon annealing\n",
    "batch_size = 32                 # size of batch size for training\n",
    "gamma = 0.99                    # discount factor of MDP\n",
    "eps_optim = 0.01                # epsilon parameter for optimization (improves stability of optimizer)\n",
    "alpha = 0.95                    # alpha parameter of RMSprop optimizer\n",
    "learning_rate = 0.00025         # step size for optimization process\n",
    "\n",
    "\n",
    "encode_method = 'one_hot'     # state encoding method ('one_hot' or 'positions')\n",
    "\n",
    "if encode_method is 'positions':\n",
    "    state_dim = 19  # explained in utils.encode_states\n",
    "else:   # one-hot\n",
    "    state_dim = env.observation_space.n\n",
    "\n",
    "regularization = None           # regularization may be 'regularization'\n",
    "save_fig = True     # whether to save figure of accumulated reward\n",
    "save_model = True      # whether to save the DQN model\n",
    "\n",
    "# Define 2-layered architecture\n",
    "architecture = {\"state_dim\": state_dim,\n",
    "                \"hidden_units\": hidden_units,\n",
    "                \"num_actions\": env.action_space.n}\n",
    "# Pack the epsilon greedy exploration parameters\n",
    "exploration_params = {\"timesteps\": schedule_timesteps, \"final_eps\": eps}\n",
    "\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ConfigParser()\n",
    "parser.read('config.ini')\n",
    "demonstrations_location_args = Dict(parser,'demonstrations_location',True)\n",
    "agent_args = Dict(parser,\"ppo\")\n",
    "discriminator_args = Dict(parser,\"eairl\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device='cpu'\n",
    "if False:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    writer = SummaryWriter()\n",
    "else:\n",
    "    writer = None\n",
    "discriminator = EAIRL(writer, device, state_dim, action_dim, discriminator_args)\n",
    "algorithm = PPO(device, state_dim, action_dim, agent_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PPO(\n",
       "  (actor): Actor(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=500, out_features=64, bias=True)\n",
       "      (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (last_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       "  (critic): Critic(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=500, out_features=64, bias=True)\n",
       "      (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (last_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ppo(env, agent, writer, max_episodes=500):\n",
    "    for episode in range(max_episodes):\n",
    "        state,_ = env.reset()\n",
    "        state=encode_states([state], encode_method, state_dim)\n",
    "        states, actions, rewards, next_states, done_masks, log_probs = [], [], [], [], [], []\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            state_tensor = torch.tensor([state], dtype=torch.float32, device=device)\n",
    "\n",
    "            # print(state_tensor)\n",
    "            action, log_prob = agent.get_action(state_tensor)\n",
    "            next_state, reward, done, terminated,_  = env.step(action)\n",
    "            next_state=encode_states([next_state], encode_method, state_dim)\n",
    "            \n",
    "            # Collect transition data\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            done_masks.append(0 if done else 1)\n",
    "            log_probs.append(log_prob.item())\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "        # Store episode reward in TensorBoard\n",
    "        writer.add_scalar(\"Episode Reward\", episode_reward, episode)\n",
    "\n",
    "        # Process batch data\n",
    "        states = torch.tensor(states, dtype=torch.float32, device=device)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64, device=device).unsqueeze(-1)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32, device=device).unsqueeze(-1)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32, device=device)\n",
    "        done_masks = torch.tensor(done_masks, dtype=torch.float32, device=device).unsqueeze(-1)\n",
    "        log_probs = torch.tensor(log_probs, dtype=torch.float32, device=device).unsqueeze(-1)\n",
    "\n",
    "        # Train the PPO agent with collected data\n",
    "        agent.train_network(writer, episode, states, actions, rewards, next_states, done_masks, log_probs)\n",
    "        \n",
    "\n",
    "        if episode  :\n",
    "            print(f\"Episode {episode}, Reward: {episode_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ppo(env,algorithm,writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mappo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
