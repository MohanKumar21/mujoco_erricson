{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.0 (SDL 2.28.4, Python 3.12.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Discrete(500), Discrete(6))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(189, {'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[110, 109, 106],\n",
       "        [110, 109, 106],\n",
       "        [124, 122, 122],\n",
       "        ...,\n",
       "        [108, 111, 109],\n",
       "        [108, 111, 109],\n",
       "        [118, 119, 119]],\n",
       "\n",
       "       [[110, 109, 106],\n",
       "        [110, 109, 106],\n",
       "        [124, 122, 122],\n",
       "        ...,\n",
       "        [108, 111, 109],\n",
       "        [108, 111, 109],\n",
       "        [118, 119, 119]],\n",
       "\n",
       "       [[114, 116, 115],\n",
       "        [114, 116, 115],\n",
       "        [126, 127, 126],\n",
       "        ...,\n",
       "        [112, 113, 111],\n",
       "        [112, 113, 111],\n",
       "        [118, 117, 115]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[116, 115, 116],\n",
       "        [116, 115, 116],\n",
       "        [106, 107, 108],\n",
       "        ...,\n",
       "        [113, 115, 114],\n",
       "        [113, 115, 114],\n",
       "        [117, 114, 117]],\n",
       "\n",
       "       [[116, 115, 116],\n",
       "        [116, 115, 116],\n",
       "        [106, 107, 108],\n",
       "        ...,\n",
       "        [113, 115, 114],\n",
       "        [113, 115, 114],\n",
       "        [117, 114, 117]],\n",
       "\n",
       "       [[115, 112, 112],\n",
       "        [115, 112, 112],\n",
       "        [119, 119, 117],\n",
       "        ...,\n",
       "        [123, 119, 118],\n",
       "        [123, 119, 118],\n",
       "        [114, 114, 117]]], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RL agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:28: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "<>:28: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/RL-main/lib/python3.12/site-packages/gym/envs/registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "  fn()\n",
      "/Users/akranthreddy/XRL Research/mujoco_erricson/Taxi/deep_q_learner.py:69: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "  if regularization is 'dropout':\n",
      "/Users/akranthreddy/XRL Research/mujoco_erricson/Taxi/deep_q_learner.py:190: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "  if regularization is 'l1':\n",
      "/var/folders/1z/_nyrys7d5bl9btjb4pm8l71w0000gn/T/ipykernel_69808/3650767101.py:28: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "  if encode_method is 'positions':\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Imports\n",
    "import gym\n",
    "import torch.optim as optim\n",
    "from utils import OptimizerSpec\n",
    "import time\n",
    "from deep_q_learner import deep_Q_learning\n",
    "\n",
    "\n",
    "# Import Taxi environment\n",
    "env = gym.make('Taxi-v3')\n",
    "\n",
    "# Define all the model parameters\n",
    "hidden_units = 32               # num. units in hidden layer\n",
    "replay_buffer_size = 200000     # buffer size\n",
    "start_learning = 50000          # num. transitions before start learning\n",
    "target_update_freq = 10000      # num. transitions between Q_target network updates\n",
    "eps = 0.1                       # final epsilon for epsilon-greedy action selection\n",
    "schedule_timesteps = 350000     # num. transitions for epsilon annealing\n",
    "batch_size = 32                 # size of batch size for training\n",
    "gamma = 0.99                    # discount factor of MDP\n",
    "eps_optim = 0.01                # epsilon parameter for optimization (improves stability of optimizer)\n",
    "alpha = 0.95                    # alpha parameter of RMSprop optimizer\n",
    "learning_rate = 0.00025         # step size for optimization process\n",
    "\n",
    "\n",
    "encode_method = 'one_hot'     # state encoding method ('one_hot' or 'positions')\n",
    "\n",
    "if encode_method is 'positions':\n",
    "    state_dim = 19  # explained in utils.encode_states\n",
    "else:   # one-hot\n",
    "    state_dim = env.observation_space.n\n",
    "\n",
    "regularization = None           # regularization may be 'regularization'\n",
    "save_fig = False        # whether to save figure of accumulated reward\n",
    "save_model = False      # whether to save the DQN model\n",
    "\n",
    "# Define 2-layered architecture\n",
    "architecture = {\"state_dim\": state_dim,\n",
    "                \"hidden_units\": hidden_units,\n",
    "                \"num_actions\": env.action_space.n}\n",
    "# Pack the epsilon greedy exploration parameters\n",
    "exploration_params = {\"timesteps\": schedule_timesteps, \"final_eps\": eps}\n",
    "# Define optimization procedure\n",
    "optimizer_spec = OptimizerSpec(\n",
    "        constructor=optim.RMSprop,\n",
    "        kwargs=dict(lr=learning_rate, alpha=alpha, eps=eps_optim),)\n",
    "        # kwargs=dict(lr=learning_rate),)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9,\n",
       " -1,\n",
       " False,\n",
       " False,\n",
       " {'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "env.step(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_batch: [246, 46, 398, 357, 58, 247, 374, 429, 6, 429, 378, 111, 351, 449, 311, 439, 244, 434, 143, 158, 391, 171, 298, 48, 337, 31, 132, 314, 274, 46, 48, 289]\n",
      "state_batch: [222, 127, 289, 358, 259, 427, 146, 362, 126, 103, 79, 291, 278, 343, 26, 3, 498, 143, 378, 434, 419, 91, 458, 102, 142, 191, 359, 439, 26, 151, 78, 339]\n",
      "state_batch: [153, 223, 324, 291, 92, 376, 32, 31, 438, 338, 419, 226, 51, 102, 19, 443, 446, 282, 279, 119, 299, 3, 424, 228, 298, 238, 483, 274, 77, 54, 468, 392]\n",
      "state_batch: [248, 277, 419, 231, 198, 319, 213, 146, 243, 259, 139, 91, 491, 368, 359, 299, 272, 492, 491, 158, 51, 276, 98, 128, 239, 429, 99, 311, 11, 128, 123, 422]\n",
      "state_batch: [483, 358, 218, 374, 234, 408, 42, 368, 74, 427, 272, 14, 372, 123, 239, 426, 12, 91, 18, 51, 489, 126, 82, 296, 474, 19, 277, 231, 42, 272, 43, 23]\n",
      "state_batch: [172, 51, 317, 374, 72, 74, 311, 311, 423, 343, 134, 398, 472, 374, 387, 438, 456, 319, 9, 26, 51, 456, 334, 122, 31, 27, 358, 214, 492, 46, 179, 74]\n",
      "state_batch: [98, 454, 314, 151, 427, 163, 26, 166, 472, 246, 394, 206, 243, 348, 227, 38, 306, 494, 406, 14, 431, 103, 308, 366, 131, 199, 106, 83, 93, 278, 118, 359]\n",
      "state_batch: [348, 452, 468, 429, 494, 334, 171, 252, 402, 11, 408, 391, 89, 267, 31, 206, 439, 214, 134, 377, 374, 467, 248, 137, 34, 349, 228, 211, 246, 11, 459, 258]\n",
      "state_batch: [471, 59, 298, 466, 496, 449, 158, 158, 119, 358, 306, 71, 343, 156, 19, 234, 256, 329, 34, 111, 252, 56, 492, 453, 146, 343, 462, 39, 314, 498, 59, 139]\n",
      "state_batch: [292, 254, 2, 283, 266, 358, 312, 26, 351, 209, 362, 311, 406, 371, 91, 271, 111, 133, 322, 348, 127, 49, 232, 396, 226, 229, 392, 391, 174, 357, 238, 229]\n",
      "state_batch: [479, 266, 118, 283, 312, 489, 379, 126, 331, 348, 171, 471, 471, 123, 106, 2, 103, 39, 454, 431, 451, 59, 457, 86, 298, 118, 434, 431, 363, 278, 73, 392]\n",
      "state_batch: [418, 346, 374, 7, 194, 367, 496, 363, 399, 476, 278, 407, 254, 212, 393, 32, 329, 343, 189, 498, 207, 322, 346, 372, 89, 398, 391, 189, 291, 98, 234, 363]\n",
      "state_batch: [367, 74, 438, 228, 97, 388, 127, 423, 166, 498, 298, 143, 2, 63, 311, 263, 342, 476, 446, 424, 452, 106, 166, 391, 311, 218, 123, 91, 371, 203, 289, 432]\n",
      "state_batch: [327, 103, 3, 246, 452, 51, 498, 126, 408, 468, 391, 248, 71, 362, 138, 63, 371, 319, 28, 338, 359, 406, 458, 194, 163, 271, 472, 71, (146, {'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)}), 162, 262, 363]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (32,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m t_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Run deep Q-learning algorithm\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdeep_Q_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marchitecture\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                \u001b[49m\u001b[43mexploration_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_learning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_update_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregularization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                \u001b[49m\u001b[43mencode_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_fig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m t_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal learning time: \u001b[39m\u001b[38;5;124m'\u001b[39m, t_end\u001b[38;5;241m-\u001b[39mt_start)\n",
      "File \u001b[0;32m~/XRL Research/mujoco_erricson/Taxi/deep_q_learner.py:164\u001b[0m, in \u001b[0;36mdeep_Q_learning\u001b[0;34m(env, architecture, optimizer_spec, exploration_params, replay_buffer_size, start_learning, batch_size, gamma, target_update_freq, regularization, encode_method, save_fig, save_model)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# One-hot encoding of states\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_batch:\u001b[39m\u001b[38;5;124m'\u001b[39m, state_batch)\n\u001b[0;32m--> 164\u001b[0m state_batch \u001b[38;5;241m=\u001b[39m \u001b[43mencode_states\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m next_state_batch \u001b[38;5;241m=\u001b[39m encode_states(next_state_batch, encode_method, state_dim)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# Convert numpy nd_array to torch Variables for calculation\u001b[39;00m\n",
      "File \u001b[0;32m~/XRL Research/mujoco_erricson/Taxi/utils.py:279\u001b[0m, in \u001b[0;36mencode_states\u001b[0;34m(states, encode_method, state_dim)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:   \u001b[38;5;66;03m# one-hot\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     states_encoded \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((batch_size, state_dim))\n\u001b[0;32m--> 279\u001b[0m     \u001b[43mstates_encoded\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m states_encoded\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (32,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "t_start = time.time()\n",
    "# Run deep Q-learning algorithm\n",
    "deep_Q_learning(env, architecture, optimizer_spec,\n",
    "                exploration_params, replay_buffer_size, start_learning,\n",
    "                batch_size, gamma, target_update_freq, regularization,\n",
    "                encode_method, save_fig, save_model)\n",
    "\n",
    "t_end = time.time()\n",
    "print('Total learning time: ', t_end-t_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL-main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
