{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SerializedBuffer:\n",
    "    def __init__(self, path, device=\"mps\"):\n",
    "        '''Path is the file location to the serialized buffer'''\n",
    "        tmp = torch.load(path)\n",
    "        self.buffer_size = self._n = tmp['state'].size(0)\n",
    "        self.device = device\n",
    "        self.states = tmp['state'].clone().to(self.device)\n",
    "        self.actions = tmp['action'].clone().to(self.device)\n",
    "        self.rewards = tmp['reward'].clone().to(self.device)\n",
    "        self.dones = tmp['done'].clone().to(self.device)\n",
    "        self.next_states = tmp['next_state'].clone().to(self.device)\n",
    "\n",
    "    def sample(self, bath_size):\n",
    "        idx = np.random.randint(low = 0, high = self._n, size = bath_size)\n",
    "        return (\n",
    "            self.states[idx],\n",
    "            self.actions[idx],\n",
    "            self.rewards[idx],\n",
    "            self.dones[idx],\n",
    "            self.next_states[idx]\n",
    "        )\n",
    "\n",
    "class Buffer(SerializedBuffer):\n",
    "\n",
    "    def __init__(self, buffer_size, state_shape, action_shape, device):\n",
    "        self._n = 0\n",
    "        self._p = 0\n",
    "        self.buffer_size = buffer_size\n",
    "        self.device = device\n",
    "\n",
    "        self.states = torch.empty(\n",
    "            (buffer_size, *state_shape), dtype=torch.float, device=device)\n",
    "        self.actions = torch.empty(\n",
    "            (buffer_size, *action_shape), dtype=torch.float, device=device)\n",
    "        self.rewards = torch.empty(\n",
    "            (buffer_size, 1), dtype=torch.float, device=device)\n",
    "        self.dones = torch.empty(\n",
    "            (buffer_size, 1), dtype=torch.float, device=device)\n",
    "        self.next_states = torch.empty(\n",
    "            (buffer_size, *state_shape), dtype=torch.float, device=device)\n",
    "\n",
    "    def append(self, state, action, reward, done, next_state):\n",
    "        self.states[self._p].copy_(torch.from_numpy(state))\n",
    "        self.actions[self._p].copy_(torch.from_numpy(action))\n",
    "        self.rewards[self._p] = float(reward)\n",
    "        self.dones[self._p] = float(done)\n",
    "        self.next_states[self._p].copy_(torch.from_numpy(next_state))\n",
    "\n",
    "        self._p = (self._p + 1) % self.buffer_size\n",
    "        self._n = min(self._n + 1, self.buffer_size)\n",
    "\n",
    "    def save(self, path):\n",
    "        if not os.path.exists(os.path.dirname(path)):\n",
    "            os.makedirs(os.path.dirname(path))\n",
    "\n",
    "        torch.save({\n",
    "            'state': self.states.clone().cpu(),\n",
    "            'action': self.actions.clone().cpu(),\n",
    "            'reward': self.rewards.clone().cpu(),\n",
    "            'done': self.dones.clone().cpu(),\n",
    "            'next_state': self.next_states.clone().cpu(),\n",
    "        }, path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.logger.set_level(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'normalization is used to train the neural network better\\nbut there is ome problem with this wrapper need to look more \\ndeep to fix it. moving on for now.\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_env(env_id):\n",
    "    return gym.make(env_id)\n",
    "\n",
    "'''normalization is used to train the neural network better\n",
    "but there is ome problem with this wrapper need to look more \n",
    "deep to fix it. moving on for now.\n",
    "'''\n",
    "# class NormalizedEnv(gym.Wrapper):\n",
    "\n",
    "#     def __init__(self, env):\n",
    "#         gym.Wrapper.__init__(self, env)\n",
    "#         self._max_episode_steps = env._max_episode_steps\n",
    "\n",
    "#         self.scale = env.action_space.n\n",
    "#         self.action_space.n /= self.scale\n",
    "#         self.action_space.low /= self.scale\n",
    "\n",
    "#     def step(self, action):\n",
    "#         return self.env.step(action * self.scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1z/_nyrys7d5bl9btjb4pm8l71w0000gn/T/ipykernel_2663/2515793457.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  algo = torch.load(\"../works_ig_3000.pth\", map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "algo = torch.load(\"../works_ig_3000.pth\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def soft_update(target, source, tau):\n",
    "    for t, s in zip(target.parameters(), source.parameters()):\n",
    "        t.data.mul_(1.0 - tau)\n",
    "        t.data.add_(tau * s.data)\n",
    "\n",
    "\n",
    "def disable_gradient(network):\n",
    "    for param in network.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "def add_random_noise(action, std):\n",
    "    action += np.random.randn(*action.shape) * std\n",
    "    return action.clip(-1.0, 1.0)\n",
    "\n",
    "\n",
    "def collect_demo(env, algo, buffer_size, device, std, p_rand, seed=0):\n",
    "    # env.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    buffer = Buffer(\n",
    "        buffer_size=buffer_size,\n",
    "        state_shape=env.observation_space.shape,\n",
    "        action_shape=env.action_space.shape,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    total_return = 0.0\n",
    "    num_episodes = 0\n",
    "\n",
    "    state,_ = env.reset()\n",
    "    t = 0\n",
    "    episode_return = 0.0\n",
    "\n",
    "    for _ in tqdm(range(1, buffer_size + 1)):\n",
    "        t += 1\n",
    "\n",
    "        if np.random.rand() < p_rand:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            print(state)\n",
    "            action = algo(state)\n",
    "            action = add_random_noise(action, std)\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        mask = False if t == env._max_episode_steps else done\n",
    "        buffer.append(state, action, reward, mask, next_state)\n",
    "        episode_return += reward\n",
    "\n",
    "        if done:\n",
    "            num_episodes += 1\n",
    "            total_return += episode_return\n",
    "            state = env.reset()\n",
    "            t = 0\n",
    "            episode_return = 0.0\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    print(f'Mean return of the expert is {total_return / num_episodes}')\n",
    "    return buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1z/_nyrys7d5bl9btjb4pm8l71w0000gn/T/ipykernel_2663/328809113.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  algo.load_state_dict(torch.load(\"../works_ig_3000.pth\", map_location=\"cpu\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo = DQN(500, 6)\n",
    "algo.load_state_dict(torch.load(\"../works_ig_3000.pth\", map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 10000\n",
    "device = \"mps\"\n",
    "std = 0.1\n",
    "p_rand = 0.3\n",
    "seed = 420\n",
    "env_id = \"Taxi-v3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m buffer \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_demo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mp_rand\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp_rand\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m buffer\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuffers\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     12\u001b[0m     env_id,\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbuffer_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_std\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_prand\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp_rand\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     14\u001b[0m ))\n",
      "Cell \u001b[0;32mIn[33], line 49\u001b[0m, in \u001b[0;36mcollect_demo\u001b[0;34m(env, algo, buffer_size, device, std, p_rand, seed)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(state)\n\u001b[0;32m---> 49\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43malgo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     action \u001b[38;5;241m=\u001b[39m add_random_noise(action, std)\n\u001b[1;32m     52\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/RL-main/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/RL-main/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[43], line 12\u001b[0m, in \u001b[0;36mDQN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 12\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x))\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/RL-main/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/RL-main/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/RL-main/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not int"
     ]
    }
   ],
   "source": [
    "buffer = collect_demo(\n",
    "        env=env,\n",
    "        algo=algo,\n",
    "        buffer_size=buffer_size,\n",
    "        device=device,\n",
    "        std=std,\n",
    "        p_rand=p_rand,\n",
    "        seed=seed\n",
    ")\n",
    "buffer.save(os.path.join(\n",
    "    'buffers',\n",
    "    env_id,\n",
    "    f'size{buffer_size}_std{std}_prand{p_rand}.pth'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer1.weight',\n",
       "              tensor([[ 0.0139, -0.0546, -0.0773,  ..., -0.0972,  0.0198, -0.1513],\n",
       "                      [ 0.0155, -0.0044, -0.0116,  ..., -0.0708,  0.0200, -0.1557],\n",
       "                      [ 0.0070,  0.0014,  0.0011,  ..., -0.1007,  0.0269, -0.1578],\n",
       "                      ...,\n",
       "                      [-0.0047, -0.1089, -0.1138,  ..., -0.0831,  0.0488, -0.1288],\n",
       "                      [ 0.0056, -0.0413, -0.0788,  ..., -0.0944,  0.0317, -0.1185],\n",
       "                      [-0.0028, -0.0912, -0.1148,  ..., -0.0731,  0.0034,  0.0667]])),\n",
       "             ('layer1.bias',\n",
       "              tensor([-0.0939, -0.0793, -0.0906,  0.0472, -0.0955, -0.0807, -0.1067, -0.0922,\n",
       "                       0.0311, -0.0746,  0.4107, -0.0213, -0.0802, -0.0773,  0.2199,  0.2363,\n",
       "                       0.0493, -0.0834,  0.1591,  0.0537, -0.0853, -0.0834, -0.0969, -0.0997,\n",
       "                      -0.0089, -0.0876,  0.0939, -0.0954, -0.0939, -0.0995, -0.0844, -0.0543,\n",
       "                       0.0715, -0.0710, -0.0202, -0.0975, -0.0830,  0.1031, -0.0934, -0.0897,\n",
       "                      -0.0982, -0.0082, -0.0944, -0.0946, -0.0503, -0.1003, -0.0052, -0.1019,\n",
       "                      -0.0868,  0.0362,  0.3032, -0.0992, -0.0106,  0.1021, -0.0200, -0.0901,\n",
       "                       0.0843,  0.0719, -0.0887, -0.0703, -0.0204, -0.0840,  0.0475, -0.0978,\n",
       "                       0.0515, -0.0905, -0.0233,  0.1746, -0.0532, -0.0878,  0.0546, -0.0201,\n",
       "                      -0.0913,  0.1601,  0.0361,  0.1559, -0.0687, -0.0833, -0.0958, -0.0193,\n",
       "                      -0.0813, -0.0592, -0.0856,  0.1042,  0.3766,  0.2602, -0.0962,  0.0837,\n",
       "                       0.1760, -0.0944, -0.0880, -0.0581, -0.0939,  0.1450,  0.0241,  0.0989,\n",
       "                      -0.0955, -0.0110, -0.0832,  0.0360, -0.0828,  0.0853, -0.0669,  0.0547,\n",
       "                      -0.0777, -0.0959, -0.0945, -0.0877, -0.0903, -0.1050, -0.1020, -0.0859,\n",
       "                      -0.0926,  0.2277, -0.0844, -0.1071,  0.0335,  0.0816, -0.0832,  0.1032,\n",
       "                      -0.0920,  0.1249, -0.0794, -0.0910,  0.0165, -0.0241, -0.1019,  0.0352])),\n",
       "             ('layer2.weight',\n",
       "              tensor([[-0.0198, -0.3458, -0.2704,  ...,  0.6578, -0.1496,  0.2290],\n",
       "                      [ 0.3210,  0.5055,  0.5560,  ..., -0.0198,  0.4449,  0.0538],\n",
       "                      [ 0.4264,  0.2939,  0.3232,  ...,  0.3908,  0.4920,  0.2999],\n",
       "                      ...,\n",
       "                      [-0.3712,  0.1472,  0.2096,  ...,  0.0564,  0.2133,  0.1888],\n",
       "                      [-0.0854, -0.2698, -0.1813,  ...,  0.6540, -0.1837,  0.2165],\n",
       "                      [ 0.4029,  0.1576,  0.1984,  ...,  0.0363,  0.2218,  0.2131]])),\n",
       "             ('layer2.bias',\n",
       "              tensor([ 0.2791, -0.0560, -0.2332, -0.0252, -0.0386, -0.0785, -0.0745, -0.0238,\n",
       "                       0.3805, -0.0518, -0.0246,  0.6804, -0.1840, -0.1409, -0.1666,  0.1540,\n",
       "                      -0.0640, -0.1220, -0.0852, -0.0397, -0.3239, -0.1151, -0.0881, -0.1638,\n",
       "                      -0.0197, -0.0387, -0.1250, -0.1156, -0.3548, -0.1678, -0.4078, -0.1262,\n",
       "                      -0.0598, -0.1361, -0.0834, -0.4369,  0.5194, -0.2218, -0.0681, -0.1866,\n",
       "                      -0.0749, -0.0354, -0.2138, -0.1114,  0.8229, -0.2028,  1.1117,  0.0360,\n",
       "                       0.1067, -0.0240,  0.4716, -0.0534, -0.0907, -0.0244, -0.0623,  0.1770,\n",
       "                      -0.4600, -0.2407, -0.0215, -0.0635, -0.1203, -0.0923, -0.0323, -0.0286,\n",
       "                      -0.1778, -0.0931, -0.1175, -0.1359, -0.0904, -0.0917,  0.2500, -0.0674,\n",
       "                      -0.1232, -0.0207,  0.4389,  0.1470, -0.0377, -0.1384,  0.2443, -0.0882,\n",
       "                       0.6263, -0.1156, -0.0941, -0.1155, -0.1979, -0.0797, -0.0180, -0.0240,\n",
       "                      -0.0484, -0.2765, -0.1554, -0.0735, -0.1836,  1.1020,  0.3634, -0.0908,\n",
       "                      -0.1421,  0.1515, -0.0713, -0.0064, -0.0971, -0.2492, -0.1090, -0.1258,\n",
       "                       1.0105, -0.1078,  0.1233, -0.0966, -0.3272, -0.4144,  0.0440, -0.1497,\n",
       "                      -0.0402, -0.2856, -0.3363, -0.1339,  0.9418, -0.1649, -0.0246, -0.0644,\n",
       "                       0.7391, -0.1994, -0.1323, -0.0362, -0.0287,  0.3721,  0.2333, -0.0886])),\n",
       "             ('layer3.weight',\n",
       "              tensor([[ 2.3190e-01, -2.6394e-01, -2.3624e-01,  9.2224e-03, -1.1802e-02,\n",
       "                       -2.8091e-01, -2.0381e-01,  3.3738e-02, -1.6647e-01, -2.1227e-01,\n",
       "                       -3.6327e-02, -9.2075e-02, -2.0090e-01, -2.2504e-01, -2.2103e-01,\n",
       "                       -1.0636e-01, -1.9999e-01, -1.7417e-01, -1.6522e-01,  2.5201e-02,\n",
       "                       -2.9185e-01, -1.8310e-01, -1.7837e-01, -2.1858e-01, -3.1363e-03,\n",
       "                       -3.1152e-02, -1.2091e-01, -1.8843e-01, -2.4789e-01, -1.5555e-01,\n",
       "                       -1.6010e-01, -1.7524e-01, -1.9807e-01, -2.1471e-01, -1.9982e-01,\n",
       "                       -1.8507e-01,  4.5764e-01, -1.2414e-01,  5.2253e-01, -1.9200e-01,\n",
       "                       -1.8241e-01, -3.7175e-02, -1.8552e-01, -2.1649e-01,  8.3208e-01,\n",
       "                       -2.5365e-01,  7.3424e-01,  8.0766e-01,  4.2550e-01,  1.3283e-02,\n",
       "                        1.2070e+00, -3.3677e-01, -1.5341e-01, -3.4244e-03, -2.3758e-01,\n",
       "                        1.1004e+00, -2.3520e-01, -2.2766e-01,  1.1236e-02, -2.2505e-01,\n",
       "                       -1.4244e-01, -1.5458e-01,  8.6159e-03, -2.2314e-03,  9.7780e-01,\n",
       "                       -2.2851e-01, -1.8467e-01, -3.2856e-01, -1.3967e-01, -1.8244e-01,\n",
       "                        1.0382e+00, -1.6935e-01, -2.0774e-01, -1.1621e-02,  9.2839e-01,\n",
       "                       -1.0881e-01,  2.8352e-02, -1.4941e-01, -2.7175e-01, -1.4762e-01,\n",
       "                       -6.4350e-01, -1.9185e-01, -1.8006e-01, -1.3471e-01, -2.0178e-01,\n",
       "                       -1.8308e-01, -1.0604e-02,  2.1710e-02, -2.1260e-01, -2.3111e-01,\n",
       "                        1.0780e+00, -1.8818e-01, -2.0663e-01,  1.0347e+00, -3.1250e-01,\n",
       "                       -2.3096e-01, -2.3850e-01, -1.5918e-01, -1.3974e-01,  2.4571e-02,\n",
       "                       -1.6692e-01, -1.6593e-01, -2.0065e-01, -2.3348e-01, -5.1917e-01,\n",
       "                       -1.4774e-01, -5.1015e-02, -2.0273e-01, -1.6800e-01, -1.6074e-01,\n",
       "                       -4.6622e-01, -1.7905e-01,  4.2178e-02,  1.7683e-01, -2.1726e-01,\n",
       "                       -2.4603e-01, -8.4693e-01, -3.2321e-01, -5.3494e-03, -2.4148e-01,\n",
       "                       -1.4332e-01, -2.4664e-01, -1.4777e-01,  3.9003e-02,  7.0548e-03,\n",
       "                       -1.3973e-01, -7.9937e-02, -1.6515e-01],\n",
       "                      [ 1.1613e+00, -3.9146e-01, -1.7543e-01,  2.4880e-02, -3.5266e-02,\n",
       "                        2.3576e-03, -1.9623e-01,  1.3331e-02,  1.3490e+00, -9.5766e-02,\n",
       "                        3.4994e-02, -3.4159e-01, -3.0661e-01, -2.5860e-01, -1.6214e-01,\n",
       "                       -4.7244e-02, -9.1370e-02, -1.6079e-01, -2.3160e-01,  3.7904e-02,\n",
       "                       -4.1434e-01, -1.3931e-01, -1.6162e-01, -2.2389e-01,  2.2008e-02,\n",
       "                        2.7341e-02, -2.0068e-01, -2.4163e-01, -2.4911e-01, -4.9722e-02,\n",
       "                       -1.8604e-01, -1.4085e-01, -2.1641e-01, -6.0256e-02, -1.9515e-01,\n",
       "                       -2.7942e-01,  1.0126e+00, -2.0697e-01,  2.2197e-02, -2.7224e-01,\n",
       "                       -1.2424e-01, -1.9300e-02, -2.6492e-01, -1.9557e-01,  3.3305e-01,\n",
       "                       -1.1763e-01,  6.6669e-01,  2.7849e-01, -1.1415e-01, -1.5225e-02,\n",
       "                        1.2367e-02, -1.7256e-01, -9.2609e-02,  2.9039e-02, -1.2426e-01,\n",
       "                        5.6497e-01, -2.2328e-01, -2.2456e-01,  1.5827e-01, -8.2915e-02,\n",
       "                       -2.5618e-01, -1.9250e-01, -3.7209e-02, -6.3673e-03, -3.8204e-03,\n",
       "                       -1.8764e-01, -1.5484e-01, -2.6820e-01, -2.4354e-01, -1.3470e-01,\n",
       "                        1.7905e-01, -1.2700e-01, -2.4171e-01,  3.8442e-02,  1.2466e+00,\n",
       "                       -3.0132e-01,  1.6293e-02, -1.0229e-01, -3.7063e-02, -1.8042e-01,\n",
       "                        9.1820e-01, -1.4693e-01, -7.5129e-02, -1.7045e-01, -1.6074e-01,\n",
       "                       -2.0653e-01, -4.2228e-02,  3.6627e-02, -2.1330e-01, -1.4637e-01,\n",
       "                       -2.8984e-01, -2.1308e-01, -2.2209e-01,  7.1625e-01, -3.0573e-02,\n",
       "                       -2.1054e-01, -2.2894e-01,  4.2937e-01, -2.3398e-01, -2.2589e-02,\n",
       "                       -2.0065e-01, -2.7485e-01, -1.3004e-01, -1.2491e-01,  1.5743e+00,\n",
       "                       -2.0983e-01, -3.6145e-01, -6.0152e-02, -2.5460e-01, -3.0608e-01,\n",
       "                       -4.8172e-01, -2.2167e-01, -1.7357e-03, -5.2985e-01, -2.3977e-01,\n",
       "                       -1.3294e-01,  2.2300e-01, -2.7692e-01, -2.1502e-02, -9.6910e-02,\n",
       "                        1.7004e-01, -1.5997e-01, -9.8982e-02,  4.5727e-03,  3.1416e-02,\n",
       "                       -3.3307e-01,  9.3131e-01, -1.4666e-01],\n",
       "                      [-8.8929e-02, -2.1139e-01, -1.8123e-01, -3.7902e-02,  2.7385e-02,\n",
       "                       -2.2751e-02, -2.2317e-01, -1.6999e-02,  9.5234e-02, -1.2137e-01,\n",
       "                       -3.8632e-02,  2.4351e-01, -9.5973e-02, -2.5597e-01, -1.0330e-01,\n",
       "                        7.8489e-02, -1.3065e-01, -2.2181e-01, -2.1740e-01, -1.7282e-02,\n",
       "                       -3.9649e-01, -1.7280e-01, -1.2743e-01, -2.5711e-01,  2.0441e-02,\n",
       "                        2.4738e-02, -1.7363e-01, -2.3071e-01, -2.0088e-01, -1.5053e-01,\n",
       "                       -1.4233e-01, -1.9259e-01, -1.9223e-01, -1.1339e-01, -1.8810e-01,\n",
       "                       -3.3264e-01,  9.6156e-02, -1.8985e-01, -7.3541e-01, -2.3034e-01,\n",
       "                       -2.0639e-01, -3.1022e-02, -1.8012e-01, -1.4984e-01,  1.7790e+00,\n",
       "                       -2.1647e-01,  8.2234e-01, -2.4030e-01, -4.9655e-01,  1.8577e-02,\n",
       "                        1.0923e+00, -3.2709e-01, -1.7799e-01,  5.7564e-03, -1.9391e-01,\n",
       "                       -5.0372e-01, -1.9557e-01, -2.1339e-01, -7.6988e-02, -3.4972e-02,\n",
       "                       -2.4174e-01, -2.2247e-01,  3.8226e-02, -1.1555e-03, -4.9582e-01,\n",
       "                       -1.5909e-01, -1.4980e-01, -3.6062e-01, -1.9511e-01, -1.7354e-01,\n",
       "                        7.5420e-01, -1.4389e-01, -2.2517e-01, -1.8961e-02,  4.6185e-01,\n",
       "                       -8.0601e-02, -9.0704e-03, -1.4572e-01, -1.0703e-01, -1.1238e-01,\n",
       "                        9.4221e-01, -2.0582e-01, -9.5657e-02, -1.6997e-01, -1.9339e-01,\n",
       "                       -1.6189e-01,  3.5186e-02,  3.4470e-02, -1.1509e-01, -2.3156e-01,\n",
       "                       -3.9528e-01, -1.5885e-01, -1.8975e-01,  1.8637e+00, -1.5797e-01,\n",
       "                       -1.8598e-01, -1.9008e-01, -3.0111e-01, -1.9107e-01,  1.2629e-02,\n",
       "                       -1.7867e-01, -3.3698e-02, -2.2419e-01, -1.9386e-01,  1.4082e+00,\n",
       "                       -2.1391e-01,  4.2947e-02, -1.5319e-01, -2.6205e-01, -2.2796e-01,\n",
       "                       -3.3775e-01, -2.0069e-01,  4.0883e-02, -3.3834e-01, -2.1683e-01,\n",
       "                       -1.9018e-01,  4.5398e-01, -4.7837e-01,  1.4599e-02, -1.3975e-01,\n",
       "                        1.8179e-01, -1.7995e-01, -1.6623e-01,  7.5353e-03, -2.1117e-02,\n",
       "                       -6.5147e-01, -2.3059e-01, -4.3012e-02],\n",
       "                      [-7.6445e-02, -2.6518e-01, -2.0957e-01, -3.1411e-04, -1.5677e-02,\n",
       "                       -8.7154e-02, -2.2593e-01, -3.8578e-03, -1.9265e-01, -2.4038e-01,\n",
       "                       -5.4798e-03,  7.7615e-01, -1.4336e-01, -1.8161e-01, -2.2385e-01,\n",
       "                        5.4077e-02, -1.8335e-01, -1.8190e-01, -2.4731e-01, -1.3386e-03,\n",
       "                       -4.9049e-01, -2.6559e-01, -2.4014e-01, -2.6616e-01,  3.1084e-02,\n",
       "                       -1.2055e-02, -1.6239e-01, -1.8710e-01, -1.9758e-01, -1.9376e-01,\n",
       "                       -1.5150e-01, -1.9263e-01, -2.2342e-01, -1.5754e-01, -2.1063e-01,\n",
       "                       -2.2092e-01, -6.7619e-01, -1.0088e-01,  4.7460e-01, -2.6001e-01,\n",
       "                       -1.3291e-01,  3.2535e-02, -2.5711e-01, -2.0477e-01, -2.5366e-01,\n",
       "                       -1.5347e-01,  2.1050e+00, -2.5574e-01,  3.1050e-01, -6.6924e-04,\n",
       "                       -4.7403e-02, -1.7807e-01, -1.2145e-01, -5.8269e-03, -2.3595e-01,\n",
       "                        3.8880e-01, -2.1829e-01, -2.2299e-01,  3.6241e-02, -5.3379e-02,\n",
       "                       -2.0849e-01, -1.3994e-01, -1.8802e-03, -1.5886e-02, -1.2483e-01,\n",
       "                       -2.0916e-01, -1.7177e-01, -3.5984e-01, -1.9508e-01, -1.4501e-01,\n",
       "                       -9.0915e-02, -1.8333e-01, -2.1574e-01, -8.3137e-03, -1.1559e-01,\n",
       "                       -3.8781e-02, -1.5548e-02, -1.4252e-01, -3.0533e-01, -2.3088e-01,\n",
       "                       -6.0202e-01, -2.4574e-01, -1.5834e-01, -2.3420e-01, -1.4020e-01,\n",
       "                       -2.1699e-01, -4.7771e-03, -2.2844e-02, -2.2609e-01, -2.2045e-01,\n",
       "                       -2.1739e-01, -1.4618e-01, -2.1513e-01,  6.2380e-01, -9.2237e-02,\n",
       "                       -1.5982e-01, -2.4382e-01, -3.9662e-01, -2.3477e-01, -2.9674e-02,\n",
       "                       -2.1850e-01, -4.0101e-02, -2.0580e-01, -2.4013e-01,  8.3756e-01,\n",
       "                       -1.4071e-01, -1.5581e-01, -2.0798e-01, -1.7906e-01, -2.4958e-01,\n",
       "                       -3.9723e-01, -2.1798e-01, -1.6472e-03, -4.6695e-03, -2.1395e-01,\n",
       "                       -2.4380e-01,  7.9307e-01, -1.0269e-01,  1.1405e-02, -1.8383e-01,\n",
       "                        7.7229e-01, -1.7930e-01, -2.3548e-01,  3.2897e-02,  1.4448e-03,\n",
       "                        9.9555e-01, -2.0850e-01, -2.2854e-01],\n",
       "                      [-7.4672e-01,  2.3042e-01, -2.2411e-01, -3.5279e-02,  9.7684e-03,\n",
       "                       -2.3411e-01, -1.2520e-01,  9.3693e-03,  6.7962e-02, -1.5603e-01,\n",
       "                       -4.2700e-04,  8.9194e-01, -1.9615e-01, -2.2292e-01, -3.0488e-01,\n",
       "                       -7.7263e-01, -1.1718e-01, -1.7382e-01, -1.0431e-01, -3.0933e-02,\n",
       "                       -4.1069e-01, -1.0831e-01, -1.2264e-01, -1.7800e-01,  1.6027e-02,\n",
       "                       -1.4211e-03, -1.5880e-01, -1.7571e-01, -1.6836e-01, -2.7430e-01,\n",
       "                       -7.5917e-01, -1.4135e-01, -9.8622e-02, -1.9932e-01, -1.5241e-01,\n",
       "                       -6.2732e-01, -2.7800e-01, -4.9240e-01, -6.4522e-01, -1.4388e-01,\n",
       "                       -1.1314e-01, -1.5370e-02, -1.4782e-01, -1.9420e-01,  4.9913e-01,\n",
       "                       -1.9855e-01,  1.1479e+00, -2.3524e-01,  4.3599e-01, -2.3432e-02,\n",
       "                        5.8076e-01,  2.7392e-01, -1.1723e-01,  3.6223e-02, -1.2819e-01,\n",
       "                       -4.6243e-01, -5.9420e-01, -1.4328e-01,  1.5995e-01, -2.2070e-01,\n",
       "                       -1.2475e-01, -1.0859e-01,  8.6587e-03, -3.9078e-02, -6.2976e-01,\n",
       "                       -1.3311e-01, -1.8487e-01,  1.1487e-01, -1.2844e-01, -1.3705e-01,\n",
       "                        2.3711e-01, -1.4625e-01, -1.5823e-01, -1.2562e-02, -1.6112e-01,\n",
       "                        1.7352e-02, -2.5801e-02, -1.5937e-01,  9.9449e-02, -1.8495e-01,\n",
       "                        1.7762e-01, -1.7650e-01, -2.0141e-01, -1.6919e-01, -1.7737e-01,\n",
       "                       -1.3477e-01,  1.4516e-03,  3.7381e-02, -1.4718e-01, -2.7917e-01,\n",
       "                       -1.7732e-01, -1.4688e-01, -1.4182e-01,  5.5999e-01,  1.1087e-01,\n",
       "                       -1.6753e-01, -2.2681e-01, -2.6180e-01, -7.1503e-02, -3.1758e-02,\n",
       "                       -1.7583e-01, -5.1893e-01, -1.5363e-01, -1.2450e-01,  1.2463e+00,\n",
       "                       -9.8655e-02, -5.1956e-02, -9.1947e-02, -1.9843e-01, -2.8185e-01,\n",
       "                        4.5036e-01, -1.8694e-01, -3.4690e-02, -1.7515e-01, -2.2168e-01,\n",
       "                       -1.1402e-01,  4.4042e-01,  9.5196e-04, -1.8190e-02, -1.6352e-01,\n",
       "                        1.0949e+00, -1.6779e-01, -1.2337e-01, -2.6243e-02, -8.1887e-03,\n",
       "                        9.7321e-01, -2.2737e-02, -1.3436e-01],\n",
       "                      [ 4.1075e-01, -3.4788e-01, -7.7083e-02, -2.5754e-02, -5.6006e-03,\n",
       "                       -1.4591e-01, -1.2455e-01,  3.9752e-02,  4.4649e-01, -1.0736e-01,\n",
       "                       -3.1612e-02,  5.3835e-01, -7.6610e-01, -5.6314e-02, -1.5250e-01,\n",
       "                       -8.5981e-01, -1.1385e-01, -7.2597e-02, -1.1449e-01,  3.0001e-02,\n",
       "                        1.8487e-01, -1.6196e-01, -1.1140e-01, -9.9089e-02, -2.7769e-02,\n",
       "                       -3.9632e-02, -1.1408e-01, -7.4876e-02, -1.1466e-01, -8.3822e-02,\n",
       "                       -6.2807e-02, -1.2160e-01, -1.2595e-01, -1.0318e-01, -8.4537e-02,\n",
       "                        2.1805e-01,  8.4412e-01,  9.5898e-02,  7.9266e-01, -1.0566e-01,\n",
       "                       -1.1567e-01,  1.5601e-02, -1.0806e-01, -6.7315e-02, -3.9630e-01,\n",
       "                       -8.8881e-02, -1.5340e-01,  5.9984e-01,  4.7826e-01, -7.1432e-03,\n",
       "                       -2.2115e-01, -7.0635e-01, -1.4220e-01,  2.7015e-02, -1.2929e-01,\n",
       "                        7.3294e-01, -4.1897e-02, -1.1032e-01,  8.9676e-01, -1.0235e-01,\n",
       "                       -1.0756e-01, -1.0742e-01,  5.3898e-03, -2.3382e-02,  4.2368e-01,\n",
       "                       -9.3799e-02, -1.2644e-01, -3.1076e-01, -9.8900e-02, -9.9604e-02,\n",
       "                       -1.5225e-01, -1.1781e-01, -1.0156e-01,  1.9484e-03,  3.3662e-01,\n",
       "                       -9.1388e-01, -1.4913e-02, -1.3437e-01, -9.3683e-01, -1.0212e-01,\n",
       "                       -5.1205e-01, -8.6234e-02, -1.0852e-01, -8.0262e-02, -9.4764e-02,\n",
       "                       -1.0969e-01,  2.2625e-02, -1.3255e-02, -1.2502e-01, -8.5281e-02,\n",
       "                        6.7365e-01, -1.0871e-01, -1.0933e-01, -4.6148e-01, -9.2082e-01,\n",
       "                       -1.0185e-01, -6.6997e-02,  4.3621e-01, -1.3215e-01, -1.1477e-02,\n",
       "                       -1.0839e-01, -6.0826e-01, -9.4330e-02, -1.1703e-01,  3.5897e-03,\n",
       "                       -9.8163e-02, -1.0013e+00, -1.6188e-01, -9.3187e-02, -1.0680e-01,\n",
       "                       -4.6571e-01, -6.6755e-02,  3.3875e-02, -3.3236e-01, -7.8559e-02,\n",
       "                       -1.1179e-01,  5.6932e-01, -9.4285e-03,  1.1928e-02, -1.0448e-01,\n",
       "                        6.1699e-01, -1.0076e-01, -1.4040e-01, -7.3562e-03,  2.2228e-02,\n",
       "                        1.8401e-01,  5.0744e-01, -1.6744e-01]])),\n",
       "             ('layer3.bias',\n",
       "              tensor([-0.0410,  0.1008,  0.0789,  0.1044,  0.0258, -0.0148]))])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL-main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
