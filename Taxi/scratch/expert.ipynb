{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3nTKMboQ_k7l"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-mxba3r_k7n",
        "outputId": "8cbec38a-1e6c-431b-8101-205174a085bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<contextlib.ExitStack at 0x174e17710>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "import random\n",
        "import math\n",
        "from itertools import count\n",
        "\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krlc1kys_k7n",
        "outputId": "fd4d05ba-01ca-44ec-c95d-27e1defdc1d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4FSS5rxr_k7n"
      },
      "outputs": [],
      "source": [
        "Transition = namedtuple('Transition', ('state','action','next_state','reward'))\n",
        "\n",
        "class ReplayMemory(object):\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen = capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dCvVYEU0_k7n"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, n_observations, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        self.layer1 = nn.Linear(n_observations, 128)\n",
        "        self.layer2 = nn.Linear(128, 128)\n",
        "        self.layer3 = nn.Linear(128, n_actions)\n",
        "\n",
        "    # Called with either one element to determine next action, or a batch\n",
        "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        return self.layer3(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WHVrtKGw_k7n"
      },
      "outputs": [],
      "source": [
        "def one_hot_encoding(x):\n",
        "    ans = torch.zeros(1,500)\n",
        "    ans[0,x] = 1\n",
        "    return ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "R9gc9PFO_k7n"
      },
      "outputs": [],
      "source": [
        "def select_action(state):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END)*math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "\n",
        "    if sample>eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            return policy_net(state).max(1).indices.view(1,1)\n",
        "\n",
        "    else:\n",
        "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sSjQHyqF_k7o"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_durations(i_episode, show_results = False):\n",
        "    plt.figure(1)\n",
        "    durations_t = torch.tensor(episode_durations, dtype = torch.float)\n",
        "    tile = \"Training \" + str(i_episode)\n",
        "    if(show_results):\n",
        "        plt.title('Results')\n",
        "    else:\n",
        "        plt.clf() #clear the current figure\n",
        "        plt.title(tile)\n",
        "\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(durations_t.numpy())\n",
        "\n",
        "    if(len(durations_t)>=100):\n",
        "        means = durations_t.unfold(0,100,1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means)) #for intitial set\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.001)\n",
        "\n",
        "    if is_ipython:\n",
        "        if not show_results:\n",
        "            display.display(plt.gcf())\n",
        "            display.clear_output(wait=True)\n",
        "        else:\n",
        "            display.display(plt.gcf())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vsDAiIJM_k7o"
      },
      "outputs": [],
      "source": [
        "def optimize_model():\n",
        "    if len(memory)<BATCH_SIZE:\n",
        "        return\n",
        "\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    batch = Transition(*zip(*transitions)) #convert (s,a,s,r),(s1,a1,s1,r1) to (s,s1) (a,a1)....\n",
        "    # Compute a mask of non-final states and concatenate the batch elements\n",
        "    # (a final state would've been the one after which simulation ended)\n",
        "    # temp = [(s.shape,s) for s in batch.next_state if s is not None]\n",
        "    # print(temp)\n",
        "    # print(\"batch: \", len(batch.next_state))\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device = device, dtype = torch.bool)\n",
        "    # print(\"non_final_mask: \", non_final_mask)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
        "\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action).long()\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "    # print(policy_net(state_batch).shape, action_batch.shape)\n",
        "    # print(\"action_batch: \", action_batch, type(action_batch))\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    with torch.no_grad():\n",
        "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    # In-place gradient clipping\n",
        "    # torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
        "    optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_NWHKDp_k7o",
        "outputId": "6a604839-51ca-440e-ddee-6931f851b487"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50.0"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "10000/200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "tc8H71ix_k7o",
        "outputId": "397da724-68e9-452a-f657-b913f9ee3503"
      },
      "outputs": [],
      "source": [
        "# #####################Training########################\n",
        "# num_episodes = 200\n",
        "# episode_durations = []\n",
        "# BATCH_SIZE = 128\n",
        "# GAMMA = 0.99\n",
        "# EPS_START = 1.0\n",
        "# EPS_END = 0.01\n",
        "# EPS_DECAY = 5000\n",
        "# TAU = 0.001\n",
        "# LR = 1e-4\n",
        "\n",
        "# n_actions = env.action_space.n\n",
        "# state, info = env.reset()\n",
        "# n_observations = 1\n",
        "\n",
        "# policy_net = DQN(500, n_actions).to(device)\n",
        "# target_net = DQN(500, n_actions).to(device)\n",
        "# target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "# optimizer = optim.AdamW(policy_net.parameters(), lr = LR, amsgrad = True)\n",
        "# memory = ReplayMemory(10000)\n",
        "\n",
        "# steps_done = 0\n",
        "\n",
        "\n",
        "# for i_episode in range(num_episodes):\n",
        "#     state, info = env.reset()\n",
        "\n",
        "#     state = one_hot_encoding(state).to(device).view(1,500)\n",
        "#     # print(state)\n",
        "#     for t in count():\n",
        "#         action = select_action(state)\n",
        "#         observation, reward, terminated, truncated, _ = env.step(action.item())\n",
        "#         reward = torch.tensor([reward], device=device)\n",
        "#         done = terminated or truncated\n",
        "\n",
        "#         if terminated:\n",
        "#             next_state = None\n",
        "#         else:\n",
        "#             # next_state = torch.tensor(observation, dtype=torch.float32, device=device).view(1,1)\n",
        "#             next_state = one_hot_encoding(observation).to(device).view(1,500)\n",
        "\n",
        "\n",
        "#         # Store the transition in memory\n",
        "#         memory.push(state, action, next_state, reward)\n",
        "\n",
        "#         # Move to the next state\n",
        "#         state = next_state\n",
        "\n",
        "#         # Perform one step of the optimization (on the policy network)\n",
        "#         optimize_model()\n",
        "\n",
        "#         # Soft update of the target network's weights\n",
        "#         # θ′ ← τ θ + (1 −τ )θ′\n",
        "#         target_net_state_dict = target_net.state_dict()\n",
        "#         policy_net_state_dict = policy_net.state_dict()\n",
        "#         for key in policy_net_state_dict:\n",
        "#             target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
        "#         target_net.load_state_dict(target_net_state_dict)\n",
        "\n",
        "#         if done:\n",
        "#             episode_durations.append(t + 1)\n",
        "#             # plot_durations()\n",
        "#             if i_episode%100==0:\n",
        "#               plot_durations(i_episode)\n",
        "\n",
        "\n",
        "\n",
        "#             break\n",
        "\n",
        "\n",
        "# print('Complete')\n",
        "# plot_durations(0, show_results=True)\n",
        "# plt.ioff()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_policy_net = DQN(500, 6)\n",
        "device = 'mps'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/1z/_nyrys7d5bl9btjb4pm8l71w0000gn/T/ipykernel_2329/809451902.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  new_policy_net.load_state_dict(torch.load('works_ig_3000.pth', map_location=torch.device('cpu')))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DQN(\n",
              "  (layer1): Linear(in_features=500, out_features=128, bias=True)\n",
              "  (layer2): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (layer3): Linear(in_features=128, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_policy_net.load_state_dict(torch.load('works_ig_3000.pth', map_location=torch.device('cpu')))\n",
        "new_policy_net.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DQN(\n",
              "  (layer1): Linear(in_features=500, out_features=128, bias=True)\n",
              "  (layer2): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (layer3): Linear(in_features=128, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_policy_net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ######################Record Video######################\n",
        "# import gymnasium as gym\n",
        "# from gym.wrappers.record_video import RecordVideo\n",
        "# import torch\n",
        "\n",
        "# # Create and wrap the environment\n",
        "# env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\n",
        "# env = RecordVideo(env, video_folder=\"./videos\", episode_trigger=lambda x: True)  # Record every episode\n",
        "\n",
        "# # Function to get action from your policy network\n",
        "# def get_action(state, policy_net):\n",
        "#     state = one_hot_encoding(state).to(device).view(1,500)\n",
        "#     with torch.no_grad():\n",
        "#         state_tensor = torch.tensor(state, dtype=torch.float32)\n",
        "#         q_values = policy_net(state_tensor)\n",
        "#         return q_values.argmax().item()\n",
        "\n",
        "# # Record a few episodes\n",
        "# num_episodes = 3\n",
        "# for episode in range(num_episodes):\n",
        "#     state = env.reset()[0]  # Get initial state\n",
        "#     done = False\n",
        "#     total_reward = 0\n",
        "    \n",
        "#     while not done:\n",
        "#         # Get action from your policy\n",
        "#         action = get_action(state, new_policy_net)\n",
        "#         # print(action, type(action))\n",
        "#         # Take step in environment\n",
        "#         next_state, reward, done, truncated, info = env.step(action)\n",
        "#         total_reward += reward\n",
        "#         state = next_state\n",
        "        \n",
        "#     print(f\"Episode {episode + 1} completed with reward: {total_reward}\")\n",
        "\n",
        "# env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AIRL Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "RL-main",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
