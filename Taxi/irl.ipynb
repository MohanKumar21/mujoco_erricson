{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.load(\"../irl/expert_data/hopper_expert_states.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:28: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "<>:28: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "/Users/ksmkumar/mohan/IRL/mujoco_erricson/Taxi/deep_q_learner.py:69: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "  if regularization is 'dropout':\n",
      "/Users/ksmkumar/mohan/IRL/mujoco_erricson/Taxi/deep_q_learner.py:190: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "  if regularization is 'l1':\n",
      "/var/folders/30/6_15k8xn4kn7vwrk6x8py9sc0000gq/T/ipykernel_6061/3283725553.py:28: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "  if encode_method is 'positions':\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import gym\n",
    "import torch.optim as optim\n",
    "from utils import OptimizerSpec\n",
    "import time\n",
    "from deep_q_learner import deep_Q_learning\n",
    "from utils import *\n",
    "\n",
    "# Import Taxi environment\n",
    "env = gym.make('Taxi-v3')\n",
    "\n",
    "# Define all the model parameters\n",
    "hidden_units = 32               # num. units in hidden layer\n",
    "replay_buffer_size = 200000     # buffer size\n",
    "start_learning = 50000          # num. transitions before start learning\n",
    "target_update_freq = 10000      # num. transitions between Q_target network updates\n",
    "eps = 0.1                       # final epsilon for epsilon-greedy action selection\n",
    "schedule_timesteps = 350000     # num. transitions for epsilon annealing\n",
    "batch_size = 32                 # size of batch size for training\n",
    "gamma = 0.99                    # discount factor of MDP\n",
    "eps_optim = 0.01                # epsilon parameter for optimization (improves stability of optimizer)\n",
    "alpha = 0.95                    # alpha parameter of RMSprop optimizer\n",
    "learning_rate = 0.00025         # step size for optimization process\n",
    "\n",
    "\n",
    "encode_method = 'one_hot'     # state encoding method ('one_hot' or 'positions')\n",
    "\n",
    "if encode_method is 'positions':\n",
    "    state_dim = 19  # explained in utils.encode_states\n",
    "else:   # one-hot\n",
    "    state_dim = env.observation_space.n\n",
    "\n",
    "regularization = None           # regularization may be 'regularization'\n",
    "save_fig = True     # whether to save figure of accumulated reward\n",
    "save_model = True      # whether to save the DQN model\n",
    "\n",
    "# Define 2-layered architecture\n",
    "architecture = {\"state_dim\": state_dim,\n",
    "                \"hidden_units\": hidden_units,\n",
    "                \"num_actions\": env.action_space.n}\n",
    "# Pack the epsilon greedy exploration parameters\n",
    "exploration_params = {\"timesteps\": schedule_timesteps, \"final_eps\": eps}\n",
    "# Define optimization procedure\n",
    "optimizer_spec = OptimizerSpec(\n",
    "        constructor=optim.RMSprop,\n",
    "        kwargs=dict(lr=learning_rate, alpha=alpha, eps=eps_optim),)\n",
    "        # kwargs=dict(lr=learning_rate),)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/30/6_15k8xn4kn7vwrk6x8py9sc0000gq/T/ipykernel_6061/1501018712.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dqn=torch.load(model_path)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "# Variables to store expert trajectories\n",
    "\n",
    "# Set the maximum number of points to collect\n",
    "num_points = 2000\n",
    "model_path=\"./trained_DQN_model\"\n",
    "dqn=torch.load(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import * \n",
    "state,_=env.reset()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.eval()\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "def select_action(state, dqn):\n",
    "    # Convert state to torch tensor\n",
    "    state=encode_states([state],encode_method,state_dim)\n",
    "    state = Variable(torch.from_numpy(state).type(dtype))\n",
    "    return int(dqn(state).data.argmax())\n",
    "select_action(state,dqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert trajectories saved with exactly 2000 points.\n"
     ]
    }
   ],
   "source": [
    "# Collect exactly 2000 points (steps)\n",
    "steps_collected = 0\n",
    "state,_= env.reset()\n",
    "np.bool8=np.bool\n",
    "states = []\n",
    "actions = []\n",
    "next_states = []\n",
    "dones = []\n",
    "\n",
    "while steps_collected < num_points:\n",
    "    # Expert selects an action using DQN\n",
    "  \n",
    "    action = select_action(state,dqn)\n",
    "    # state=encode_states([state],encode_method,state_dim)[0]\n",
    "    # Perform action in the environment\n",
    "    next_state, reward, done, _,_= env.step(action)\n",
    "    \n",
    "    # Store the expert trajectory\n",
    "    states.append(state)\n",
    "    actions.append(action)\n",
    "    next_states.append(next_state)\n",
    "    dones.append(done)\n",
    "    \n",
    "    # Move to the next state\n",
    "    state = next_state\n",
    "    steps_collected += 1\n",
    "\n",
    "    # Reset environment if done\n",
    "    if done:\n",
    "        state,_ = env.reset()\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "states = np.array(states)\n",
    "actions = np.array(actions)\n",
    "next_states = np.array(next_states)\n",
    "dones = np.array(dones)\n",
    "\n",
    "# Directory to save the expert data\n",
    "expert_data_dir = \"expert_data/\"\n",
    "state_file = expert_data_dir + \"taxi_expert_states\"\n",
    "action_file = expert_data_dir + \"taxi_expert_actions.npy\"\n",
    "next_state_file = expert_data_dir + \"taxi_expert_next_states.npy\"\n",
    "done_file = expert_data_dir + \"taxi_expert_dones.npy\"\n",
    "\n",
    "# Save the expert trajectories to .npy files\n",
    "np.save(state_file, states)\n",
    "np.save(action_file, actions)\n",
    "np.save(next_state_file, next_states)\n",
    "np.save(done_file, dones)\n",
    "\n",
    "print(f\"Expert trajectories saved with exactly {num_points} points.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "print(actions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mappo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
